import sqlite3
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import re
from datetime import datetime

DB_PATH = "oca_courses.db"
LOG_FILE = "scrape_log.txt"

def log_message(message):
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\n")

def scrape_text(url):
    try:
        response = requests.get(url, timeout=10)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "html.parser")

        # --- ã‚¿ã‚¤ãƒˆãƒ« ---
        title = soup.title.string.strip() if soup.title else "No Title"

        # --- ä¸»è¦æœ¬æ–‡æŠ½å‡º ---
        # ã¾ãšã€ã‚³ãƒ¼ã‚¹è©³ç´°éƒ¨åˆ†ï¼ˆdiv.courseDetailãªã©ï¼‰ã‚’å„ªå…ˆçš„ã«æŠ½å‡º
        main_section = (
            soup.find("div", class_="courseDetail")
            or soup.find("section", class_="course-detail")
            or soup.find("main")
            or soup.body
        )

        texts = []
        for tag in main_section.find_all(["h1", "h2", "h3", "p", "li"]):
            text = tag.get_text(" ", strip=True)
            if len(text) > 10 and not any(x in text for x in [
                "OCAå¤§é˜ªãƒ‡ã‚¶ã‚¤ãƒ³ï¼†ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼å°‚é–€å­¦æ ¡",
                "å¤§é˜ªãƒ†ãƒƒã‚¯",
                "å­¦æ ¡ç´¹ä»‹",
                "äº¤é€šã‚¢ã‚¯ã‚»ã‚¹",
                "æƒ…å ±å…¬é–‹",
                "æ–‡éƒ¨ç§‘å­¦å¤§è‡£èªå®š",
                "è·æ¥­å®Ÿè·µå°‚é–€èª²ç¨‹",
                "Â©",
                "All rights reserved"
            ]):
                texts.append(text)

        # --- ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ ---
        content = "\n".join(texts)
        content = re.sub(r"\s+", " ", content)  # ä½™åˆ†ãªæ”¹è¡Œã‚„ç©ºç™½ã‚’é™¤å»
        content = re.sub(r"(å¤§é˜ªãƒ†ãƒƒã‚¯ï½œ)?OCAâ¼¤é˜ªãƒ‡ã‚¶ã‚¤ãƒ³&ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼å°‚â¾¨å­¦æ ¡.*?æ¥­ç•Œã®ãƒ—ãƒ­ã«ãªã‚‹", "", content)
        content = re.sub(r"(OCA|å¤§é˜ªãƒ†ãƒƒã‚¯).*?(ãƒ‡ã‚¶ã‚¤ãƒ³|ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼)", "", content)
        content = content.strip()

        # --- å†…å®¹ãŒçŸ­ã™ãã‚‹å ´åˆã¯æœ¬æ–‡å…¨ä½“ã‹ã‚‰æŠ½å‡º ---
        if len(content) < 100:
            fallback_texts = [t.get_text(" ", strip=True) for t in soup.find_all("p") if len(t.get_text(strip=True)) > 20]
            content = "\n".join(fallback_texts)

        return title, content

    except Exception as e:
        print(f"âš ï¸ å–å¾—å¤±æ•—: {url} ({e})")
        return None, None



def build_database():
    """å…¨ã‚³ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦DBä½œæˆï¼ˆFTSå¯¾å¿œï¼‰"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()

    # é€šå¸¸ãƒ†ãƒ¼ãƒ–ãƒ«
    c.execute("CREATE TABLE IF NOT EXISTS courses (url TEXT PRIMARY KEY, title TEXT, content TEXT)")
    # FTSãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆå…¨æ–‡æ¤œç´¢ç”¨ï¼‰
    c.execute("CREATE VIRTUAL TABLE IF NOT EXISTS courses_fts USING fts5(title, content, url, content='courses', content_rowid='rowid')")

    conn.commit()

    base_url = "https://www.oca.ac.jp/course/"
    response = requests.get(base_url)
    soup = BeautifulSoup(response.text, "html.parser")

    links = [a["href"] for a in soup.find_all("a", href=True) if "/course/" in a["href"]]
    links = list(set([l if l.startswith("http") else f"https://www.oca.ac.jp{l}" for l in links]))

    print(f"ğŸŒ å–å¾—å¯¾è±¡: {len(links)}ä»¶")
    log_message(f"=== ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ï¼ˆ{len(links)}ä»¶ï¼‰ ===")

    failed = []

    for url in tqdm(links):
        title, content = scrape_text(url)
        if content and len(content.strip()) > 50:
            c.execute("INSERT OR REPLACE INTO courses VALUES (?, ?, ?)", (url, title, content))
        else:
            failed.append(url)

    # FTSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰
    c.execute("INSERT INTO courses_fts(courses_fts) VALUES ('rebuild')")
    conn.commit()
    conn.close()
    print("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆå®Œäº†ï¼")

    if failed:
        print(f"âš ï¸ æœ¬æ–‡ãŒå–å¾—ã§ããªã‹ã£ãŸURL: {len(failed)}ä»¶")
        for f in failed[:5]:
            print(" -", f)
        log_message(f"æœ¬æ–‡ãªã—URL: {len(failed)}ä»¶")


def search_courses(query):
    """FTSã§å…¨æ–‡æ¤œç´¢"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT title, content, url FROM courses_fts WHERE courses_fts MATCH ?", (query,))
    results = c.fetchall()
    conn.close()
    return results


def main():
    try:
        conn = sqlite3.connect(DB_PATH)
        conn.execute("SELECT 1 FROM courses LIMIT 1")
        conn.close()
    except:
        print("âš™ï¸ DBãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ(y/n): ", end="")
        if input().lower() == "y":
            build_database()
        else:
            print("çµ‚äº†ã—ã¾ã™ã€‚")
            return

    while True:
        query = input("\nğŸ’¬ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆçµ‚äº†: exitï¼‰\n> ")
        if query.lower() == "exit":
            break
        results = search_courses(query)
        if results:
            print(f"\nğŸ” {len(results)}ä»¶ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸï¼š")
            for title, content, url in results[:3]:
                snippet = re.sub(r"\s+", " ", content[:200])
                print(f"\nğŸ“˜ {title}\nğŸ§¾ {snippet}...\nğŸŒ {url}")
        else:
            print("è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


if __name__ == "__main__":
    main()